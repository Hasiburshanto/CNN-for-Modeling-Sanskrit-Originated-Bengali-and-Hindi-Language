# CNN-for-Modeling-Sanskrit-Originated-Bengali-and-Hindi-Language
Though recent works have focused on model002 ing high resource languages, the area is still 003 unexplored for low resource languages like 004 Bengali and Hindi. We propose an end-to-end 005 trainable memory efficient CNN architecture 006 named CoCNN to handle specific character007 istics such as high inflection, morphological 008 richness, flexible word order and phonetical 009 spelling errors of Bengali and Hindi. In par010 ticular, we introduce two learnable convolu011 tional sub-models at word and at sentence level 012 that are end-to-end trainable. We show that 013 state-of-the-art (SOTA) Transformer models 014 including pretrained BERT do not necessar015 ily yield the best performance for Bengali and 016 Hindi. CoCNN outperforms pretrained BERT 017 with 16X less parameters and achieves much 018 better performance than SOTA LSTMs on mul019 tiple real-world datasets. This is the first study 020 on the effectiveness of different architectures 021 from Convolution, Recurrent, and Transformer 022 neural net paradigm for modeling Bengali and 023 Hindi.
